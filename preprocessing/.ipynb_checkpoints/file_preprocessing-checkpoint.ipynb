{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a94a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e6a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"../../N-CMAPSS_DS01-005.h5\", \"../N-CMAPSS_DS02-006.h5\", \"../N-CMAPSS_DS03-012.h5\", \"../N-CMAPSS_DS04.h5\", \"../N-CMAPSS_DS05.h5\", \"../N-CMAPSS_DS06.h5\", \"../N-CMAPSS_DS07.h5\", \"../N-CMAPSS_DS08a-009.h5\", \"../N-CMAPSS_DS08c-008.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588843b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(filename):\n",
    "    with h5py.File(filename, 'r') as infile:\n",
    "        # get columns from file\n",
    "        W_var = np.array(infile.get('W_var'))\n",
    "        X_s_var = np.array(infile.get('X_s_var'))  \n",
    "        X_v_var = np.array(infile.get('X_v_var')) \n",
    "        T_var = np.array(infile.get('T_var'))\n",
    "        A_var = np.array(infile.get('A_var'))\n",
    "\n",
    "    # from np.array to list dtype U4/U5\n",
    "    W_var = list(np.array(W_var, dtype='U20'))\n",
    "    X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "    X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "    T_var = list(np.array(T_var, dtype='U20'))\n",
    "    A_var = list(np.array(A_var, dtype='U20'))\n",
    "\n",
    "    return W_var + X_s_var + X_v_var + T_var + A_var \n",
    "\n",
    "def extract_data(filename, get_cols=False):\n",
    "    with h5py.File(filename, 'r') as infile:\n",
    "        # train data\n",
    "        W_dev = np.array(infile.get('W_dev'))          \n",
    "        X_s_dev = np.array(infile.get('X_s_dev'))       \n",
    "        X_v_dev = np.array(infile.get('X_v_dev'))      \n",
    "        T_dev = np.array(infile.get('T_dev'))       \n",
    "        y_train = np.array(infile.get('Y_dev'))           \n",
    "        A_dev = np.array(infile.get('A_dev')) \n",
    "        x_train = np.concatenate((W_dev, X_s_dev, X_v_dev, T_dev, A_dev), axis=1)\n",
    "\n",
    "        # test data\n",
    "        W_test = np.array(infile.get('W_test'))        \n",
    "        X_s_test = np.array(infile.get('X_s_test'))     \n",
    "        X_v_test = np.array(infile.get('X_v_test'))      \n",
    "        T_test = np.array(infile.get('T_test'))         \n",
    "        y_test = np.array(infile.get('Y_test'))         \n",
    "        A_test = np.array(infile.get('A_test')) \n",
    "        x_test = np.concatenate((W_test, X_s_test, X_v_test, T_test, A_test), axis=1)\n",
    "\n",
    "    # concat data together for EDA\n",
    "    #X = np.concatenate((x_train, x_test), axis=0)\n",
    "    #y = np.concatenate((y_train, y_test), axis=0)\n",
    "    \n",
    "    # possibly extract columns\n",
    "    cols = None\n",
    "    if get_cols:\n",
    "        cols = extract_columns(filename)\n",
    "\n",
    "    return {\"data\": (x_train, x_test, y_train, y_test), \"columns\": cols}\n",
    "\n",
    "def read_data(files):\n",
    "    data = extract_data(files[0], get_cols=True)\n",
    "    columns = data[\"columns\"]\n",
    "    X, y = data[\"data\"]\n",
    "    for filename in files[1:]:\n",
    "        X_temp, y_temp = extract_data(filename)[\"data\"]\n",
    "        X, y = np.concatenate((X, X_temp), axis=0), np.concatenate((y, y_temp), axis=0)\n",
    "    return X, y, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb7e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(files, output_file, id_cols, chunk_size=10000):\n",
    "    train_file = output_file + \"_train.h5\"\n",
    "    test_file = output_file + \"_test.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    train_size, test_size = 0, 0\n",
    "\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        data = extract_data(filename, get_cols=i==0)  # Read one file at a time\n",
    "        X_train, X_test, y_train, y_test = data[\"data\"]\n",
    "        y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "\n",
    "        if i == 0:\n",
    "            columns = data[\"columns\"]\n",
    "            id_idxs = [columns.index(col) for col in id_cols]\n",
    "            # make starting datasets\n",
    "            h5_train.create_dataset(\"X\", shape=(0, X_train.shape[1]), maxshape=(None, X_train.shape[1]), dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_train.shape[1]))\n",
    "            h5_train.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", chunks=(chunk_size,))\n",
    "\n",
    "            h5_test.create_dataset(\"X\", shape=(0, X_test.shape[1]), maxshape=(None, X_test.shape[1]), dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_test.shape[1]))\n",
    "            h5_test.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", chunks=(chunk_size,))\n",
    "\n",
    "            h5_train.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "            h5_test.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "\n",
    "\n",
    "\n",
    "        # append data incrementally to avoid memory issues\n",
    "        # fix shapes\n",
    "        h5_train[\"X\"].resize((train_size + X_train.shape[0]), axis=0)\n",
    "        h5_train[\"X\"][train_size:] = X_train\n",
    "        h5_train[\"y\"].resize((train_size + y_train.shape[0]), axis=0)\n",
    "        h5_train[\"y\"][train_size:] = y_train\n",
    "\n",
    "        h5_test[\"X\"].resize((test_size + X_test.shape[0]), axis=0)\n",
    "        h5_test[\"X\"][test_size:] = X_test\n",
    "        h5_test[\"y\"].resize((test_size + y_test.shape[0]), axis=0)\n",
    "        h5_test[\"y\"][test_size:] = y_test\n",
    "\n",
    "        train_size += X_train.shape[0]\n",
    "        test_size += X_test.shape[0]\n",
    "\n",
    "    # cleanup\n",
    "    h5_train.close()\n",
    "    h5_test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d559cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N-CMAPSS_DS01-005.h5 (1/9)...\n",
      "Processing N-CMAPSS_DS02-006.h5 (2/9)...\n",
      "Processing N-CMAPSS_DS03-012.h5 (3/9)...\n",
      "Processing N-CMAPSS_DS04.h5 (4/9)...\n",
      "Processing N-CMAPSS_DS05.h5 (5/9)...\n",
      "Processing N-CMAPSS_DS06.h5 (6/9)...\n",
      "Processing N-CMAPSS_DS07.h5 (7/9)...\n",
      "Processing N-CMAPSS_DS08a-009.h5 (8/9)...\n",
      "Processing N-CMAPSS_DS08c-008.h5 (9/9)...\n"
     ]
    }
   ],
   "source": [
    "merge_files(files, \"engine_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901f382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(files, output_file, id_cols, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Merges multiple HDF5 files into a single train/test HDF5 dataset and creates an index.csv file.\n",
    "\n",
    "    Parameters:\n",
    "        files (list): List of HDF5 file paths to merge.\n",
    "        output_file (str): Output HDF5 file path (without extension).\n",
    "        id_cols (list): List of column names identifying a time series (e.g., [\"unit\", \"cycle\"]).\n",
    "        chunk_size (int): Number of rows to process in each batch (to prevent memory issues).\n",
    "    \"\"\"\n",
    "    train_file = output_file + \"_train.h5\"\n",
    "    test_file = output_file + \"_test.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    train_size, test_size = 0, 0\n",
    "    index_data = []  # Store (unit, cycle, start, stop, dataset_type)\n",
    "\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        data = extract_data(filename, get_cols=(i == 0))  # Read one file at a time\n",
    "        X_train, X_test, y_train, y_test = data[\"data\"]\n",
    "        y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "\n",
    "        if i == 0:\n",
    "            columns = data[\"columns\"]\n",
    "\n",
    "            # Create datasets\n",
    "            h5_train.create_dataset(\"X\", shape=(0, X_train.shape[1]), maxshape=(None, X_train.shape[1]), \n",
    "                                    dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_train.shape[1]))\n",
    "            h5_train.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", \n",
    "                                    chunks=(chunk_size,))\n",
    "\n",
    "            h5_test.create_dataset(\"X\", shape=(0, X_test.shape[1]), maxshape=(None, X_test.shape[1]), \n",
    "                                   dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_test.shape[1]))\n",
    "            h5_test.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", \n",
    "                                   chunks=(chunk_size,))\n",
    "\n",
    "            h5_train.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "            h5_test.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "\n",
    "        # ** Extract indices of ID columns **\n",
    "        id_idxs = [columns.index(col) for col in id_cols]\n",
    "\n",
    "        # ** Extract unique (unit, cycle) time series for train and test sets **\n",
    "        for dataset_type, X_data, base_index in [(\"train\", X_train, train_size), (\"test\", X_test, test_size)]:\n",
    "            if X_data.shape[0] == 0:\n",
    "                continue  # Skip if there's no data\n",
    "\n",
    "            # Load only unit and cycle columns\n",
    "            id_values = X_data[:, id_idxs]\n",
    "            id_df = pd.DataFrame(id_values, columns=id_cols)\n",
    "\n",
    "            # Get start and stop indices for each unique (unit, cycle)\n",
    "            grouped = id_df.groupby(id_cols).apply(lambda df: (df.index.min(), df.index.max()))\n",
    "            for (unit, cycle), (start, stop) in grouped.items():\n",
    "                index_data.append([unit, cycle, start + base_index, stop + base_index, dataset_type])\n",
    "\n",
    "        # ** Append data incrementally to avoid memory issues **\n",
    "        h5_train[\"X\"].resize((train_size + X_train.shape[0]), axis=0)\n",
    "        h5_train[\"X\"][train_size:] = X_train\n",
    "        h5_train[\"y\"].resize((train_size + y_train.shape[0]), axis=0)\n",
    "        h5_train[\"y\"][train_size:] = y_train\n",
    "\n",
    "        h5_test[\"X\"].resize((test_size + X_test.shape[0]), axis=0)\n",
    "        h5_test[\"X\"][test_size:] = X_test\n",
    "        h5_test[\"y\"].resize((test_size + y_test.shape[0]), axis=0)\n",
    "        h5_test[\"y\"][test_size:] = y_test\n",
    "\n",
    "        train_size += X_train.shape[0]\n",
    "        test_size += X_test.shape[0]\n",
    "\n",
    "    # ** Save the index file as CSV **\n",
    "    index_df = pd.DataFrame(index_data, columns=[\"unit\", \"cycle\", \"start_idx\", \"stop_idx\", \"dataset\"])\n",
    "    index_df.to_csv(output_file + \"_index.csv\", index=False)\n",
    "    print(f\"✅ Index file saved: {output_file}_index.csv\")\n",
    "\n",
    "    # Cleanup\n",
    "    h5_train.close()\n",
    "    h5_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84914e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N-CMAPSS_DS01-005.h5 (1/9)...\n",
      "Processing N-CMAPSS_DS02-006.h5 (2/9)...\n",
      "Processing N-CMAPSS_DS03-012.h5 (3/9)...\n",
      "Processing N-CMAPSS_DS04.h5 (4/9)...\n",
      "Processing N-CMAPSS_DS05.h5 (5/9)...\n",
      "Processing N-CMAPSS_DS06.h5 (6/9)...\n",
      "Processing N-CMAPSS_DS07.h5 (7/9)...\n",
      "Processing N-CMAPSS_DS08a-009.h5 (8/9)...\n",
      "Processing N-CMAPSS_DS08c-008.h5 (9/9)...\n",
      "✅ Index file saved: engine_index.csv\n"
     ]
    }
   ],
   "source": [
    "merge_files(files, \"engine\", id_cols=[\"unit\", \"cycle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cd7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def merge_files_timewindows(\n",
    "    files,\n",
    "    output_file,\n",
    "    id_cols,\n",
    "    window_size=50,\n",
    "    overlap=10,\n",
    "    chunk_size=10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Merges multiple HDF5 files into train/test windowed HDF5 datasets.\n",
    "    The output files contain shape (num_windows, window_size, num_features).\n",
    "\n",
    "    Parameters:\n",
    "        files (list of str): List of HDF5 file paths to merge.\n",
    "        output_file (str): Basename for output HDF5 files (no \".h5\" extension).\n",
    "                           This will produce \"{output_file}_train_windows.h5\" and\n",
    "                           \"{output_file}_test_windows.h5\".\n",
    "        id_cols (list of str): Column names identifying each time series, e.g. [\"unit\", \"cycle\"].\n",
    "        window_size (int): Size of each time window (# of observations).\n",
    "        overlap (int): Number of overlapping rows between consecutive windows.\n",
    "        chunk_size (int): Used when creating chunked HDF5 datasets. Does NOT\n",
    "                          control reading chunk size from the input files,\n",
    "                          but sets chunk shape in the output HDF5.\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1. Prepare output HDF5 files for train and test windows\n",
    "    # ----------------------------------------------------------------------\n",
    "    train_file = output_file + \"_train_windows.h5\"\n",
    "    test_file = output_file + \"_test_windows.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    # We'll create empty placeholders; we don't know the final # of windows yet.\n",
    "    X_train_dset = None\n",
    "    y_train_dset = None\n",
    "    X_test_dset = None\n",
    "    y_test_dset = None\n",
    "\n",
    "    train_count = 0  # How many train windows we've appended so far\n",
    "    test_count = 0   # How many test windows we've appended so far\n",
    "\n",
    "    # Step size for time windows (controls overlap)\n",
    "    step_size = window_size - overlap\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2. Helper function for time-window generation on a single ID group\n",
    "    # ----------------------------------------------------------------------\n",
    "    def create_time_windows(X_sub, y_sub):\n",
    "        \"\"\"\n",
    "        Given the X and y arrays for a single ID group (time-series),\n",
    "        generate all windows with shape (num_windows, window_size, num_features).\n",
    "        For the label, we take y at the *end* of each window (common RUL approach).\n",
    "        \"\"\"\n",
    "        windows_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        n_samples = X_sub.shape[0]\n",
    "        # Go up to n_samples - window_size + 1\n",
    "        for start_idx in range(0, n_samples - window_size + 1, step_size):\n",
    "            end_idx = start_idx + window_size\n",
    "            windows_list.append(X_sub[start_idx:end_idx])\n",
    "            # Label often taken from the last row in the window\n",
    "            labels_list.append(y_sub[end_idx - 1])\n",
    "\n",
    "        if len(windows_list) == 0:\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        # Stack them into final arrays\n",
    "        Xw = np.stack(windows_list, axis=0)  # shape: (num_windows, window_size, num_features)\n",
    "        yw = np.array(labels_list, dtype=Xw.dtype)  # shape: (num_windows,)\n",
    "        return Xw, yw\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3. Process each file in 'files'\n",
    "    #    We assume you have a function `extract_data(h5_path, get_cols=False)`\n",
    "    #    that returns { \"data\": (X_train, X_test, y_train, y_test), \"columns\": [...] }\n",
    "    # ----------------------------------------------------------------------\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3a. Extract the data from the source file\n",
    "        # ----------------------------------------------------------------------\n",
    "        # You need to implement or already have `extract_data`; it typically does:\n",
    "        #   X_train, X_test, y_train, y_test = ...\n",
    "        #   columns = [...]\n",
    "        #   return { \"data\": (X_train, X_test, y_train, y_test), \"columns\": columns }\n",
    "        data_dict = extract_data(filename, get_cols=True)\n",
    "        X_train_raw, X_test_raw, y_train_raw, y_test_raw = data_dict[\"data\"]\n",
    "        y_train_raw = y_train_raw.squeeze()\n",
    "        y_test_raw = y_test_raw.squeeze()\n",
    "        columns = data_dict[\"columns\"]\n",
    "        print(columns)\n",
    "\n",
    "        # If this is the first file, create the output datasets\n",
    "        if i == 0:\n",
    "            # We remove the ID columns + target from the \"feature\" set\n",
    "            id_idxs = [columns.index(col) for col in id_cols]\n",
    "            target_idx = columns.index(\"RUL\") if \"RUL\" in columns else None\n",
    "\n",
    "            feature_cols = [\n",
    "                c for c in range(len(columns))\n",
    "                if (c not in id_idxs) and (c != target_idx)\n",
    "            ]\n",
    "            num_features = len(feature_cols)\n",
    "\n",
    "            # Create placeholders for train/test window sets\n",
    "            X_train_dset = h5_train.create_dataset(\n",
    "                \"X\", shape=(0, window_size, num_features),\n",
    "                maxshape=(None, window_size, num_features),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size, window_size, num_features)\n",
    "            )\n",
    "            y_train_dset = h5_train.create_dataset(\n",
    "                \"y\", shape=(0,),\n",
    "                maxshape=(None,),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size,)\n",
    "            )\n",
    "\n",
    "            X_test_dset = h5_test.create_dataset(\n",
    "                \"X\", shape=(0, window_size, num_features),\n",
    "                maxshape=(None, window_size, num_features),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size, window_size, num_features)\n",
    "            )\n",
    "            y_test_dset = h5_test.create_dataset(\n",
    "                \"y\", shape=(0,),\n",
    "                maxshape=(None,),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size,)\n",
    "            )\n",
    "\n",
    "            # Store attribute with feature column names if you want\n",
    "            # (excluding ID + target columns)\n",
    "            kept_column_names = [columns[c] for c in feature_cols]\n",
    "            h5_train.attrs[\"columns\"] = np.array(kept_column_names, dtype=\"S\")\n",
    "            h5_test.attrs[\"columns\"] = np.array(kept_column_names, dtype=\"S\")\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3b. Group the train set by ID columns, generate windows, append them\n",
    "        # ----------------------------------------------------------------------\n",
    "        if X_train_raw.shape[0] > 0:\n",
    "            # Convert to DataFrame for grouping\n",
    "            df_train = pd.DataFrame(X_train_raw, columns=columns)\n",
    "            df_train[\"RUL\"] = y_train_raw\n",
    "\n",
    "            # For each unique ID, build windows\n",
    "            group_df_train = df_train.groupby(id_cols, sort=False)\n",
    "            for _, sub_df in group_df_train:\n",
    "                # Separate features vs. label\n",
    "                # - We'll keep only the relevant feature columns\n",
    "                # - We'll keep label from the 'RUL' column\n",
    "                sub_X = sub_df.iloc[:, feature_cols].values  # shape (n, num_features)\n",
    "                sub_y = sub_df[\"RUL\"].values\n",
    "\n",
    "                # Generate windows\n",
    "                Xw, yw = create_time_windows(sub_X, sub_y)\n",
    "                if Xw.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                # Append to HDF5\n",
    "                n_new = Xw.shape[0]\n",
    "                X_train_dset.resize(X_train_dset.shape[0] + n_new, axis=0)\n",
    "                y_train_dset.resize(y_train_dset.shape[0] + n_new, axis=0)\n",
    "\n",
    "                X_train_dset[-n_new:] = Xw\n",
    "                y_train_dset[-n_new:] = yw\n",
    "                train_count += n_new\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3c. Same logic for test set\n",
    "        # ----------------------------------------------------------------------\n",
    "        if X_test_raw.shape[0] > 0:\n",
    "            # Convert to DataFrame for grouping\n",
    "            df_test = pd.DataFrame(X_test_raw, columns=columns)\n",
    "            df_test[\"RUL\"] = y_test_raw\n",
    "\n",
    "            group_df_test = df_test.groupby(id_cols, sort=False)\n",
    "            for _, sub_df in group_df_test:\n",
    "                sub_X = sub_df.iloc[:, feature_cols].values\n",
    "                sub_y = sub_df[\"RUL\"].values\n",
    "\n",
    "                Xw, yw = create_time_windows(sub_X, sub_y)\n",
    "                if Xw.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                n_new = Xw.shape[0]\n",
    "                X_test_dset.resize(X_test_dset.shape[0] + n_new, axis=0)\n",
    "                y_test_dset.resize(y_test_dset.shape[0] + n_new, axis=0)\n",
    "\n",
    "                X_test_dset[-n_new:] = Xw\n",
    "                y_test_dset[-n_new:] = yw\n",
    "                test_count += n_new\n",
    "\n",
    "        print(f\"  ...done. Train windows so far: {train_count}, Test windows so far: {test_count}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4. Cleanup and Close\n",
    "    # ----------------------------------------------------------------------\n",
    "    h5_train.close()\n",
    "    h5_test.close()\n",
    "    print(f\"✅ Train windows file saved: {train_file}  (total windows: {train_count})\")\n",
    "    print(f\"✅ Test windows file saved: {test_file}   (total windows: {test_count})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55755d25",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerge_files_timewindows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mengine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# -> produces \"merged_engine_train_windows.h5\" and \"merged_engine_test_windows.h5\"\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcycle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\u001b[38;5;66;43;03m# windows will have step_size=45\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# chunk shape for output dataset\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmerge_files_timewindows\u001b[0;34m(files, output_file, id_cols, window_size, overlap, chunk_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m train_file \u001b[38;5;241m=\u001b[39m output_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_train_windows.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m test_file \u001b[38;5;241m=\u001b[39m output_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_test_windows.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m h5_train \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m h5_test \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(test_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# We'll create empty placeholders; we don't know the final # of windows yet.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/files.py:406\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m phil:\n\u001b[1;32m    405\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m--> 406\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_fcpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack_order\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/files.py:179\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mcreate(name, h5f\u001b[38;5;241m.\u001b[39mACC_EXCL, fapl\u001b[38;5;241m=\u001b[39mfapl, fcpl\u001b[38;5;241m=\u001b[39mfcpl)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACC_TRUNC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:108\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "merge_files_timewindows(\n",
    "    files=files,\n",
    "    output_file=\"engine\",         # -> produces \"merged_engine_train_windows.h5\" and \"merged_engine_test_windows.h5\"\n",
    "    id_cols=[\"unit\", \"cycle\"],\n",
    "    window_size=50,\n",
    "    overlap=5,                           # windows will have step_size=45\n",
    "    chunk_size=1000                      # chunk shape for output dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13402fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_hdf5(filename=\"data.h5\"):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        X = f[\"X\"][:]\n",
    "        y = f[\"y\"][:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcafa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_from_hdf5(filename=\"engine_train_windows.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b917db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980793, 50, 44)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef7495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
