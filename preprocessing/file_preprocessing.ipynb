{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a94a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e6a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"../../data/N-CMAPSS_DS01-005.h5\", \"../../data/N-CMAPSS_DS02-006.h5\", \"../../data/N-CMAPSS_DS03-012.h5\", \"../../data/N-CMAPSS_DS04.h5\", \"../../data/N-CMAPSS_DS05.h5\", \"../../data/N-CMAPSS_DS06.h5\", \"../../data/N-CMAPSS_DS07.h5\", \"../../data/N-CMAPSS_DS08a-009.h5\", \"../../data/N-CMAPSS_DS08c-008.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588843b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(filename):\n",
    "    with h5py.File(filename, 'r') as infile:\n",
    "        # get columns from file\n",
    "        W_var = np.array(infile.get('W_var'))\n",
    "        X_s_var = np.array(infile.get('X_s_var'))  \n",
    "        X_v_var = np.array(infile.get('X_v_var')) \n",
    "        T_var = np.array(infile.get('T_var'))\n",
    "        A_var = np.array(infile.get('A_var'))\n",
    "\n",
    "    # from np.array to list dtype U4/U5\n",
    "    W_var = list(np.array(W_var, dtype='U20'))\n",
    "    X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "    X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "    T_var = list(np.array(T_var, dtype='U20'))\n",
    "    A_var = list(np.array(A_var, dtype='U20'))\n",
    "\n",
    "    return W_var + X_s_var + X_v_var + T_var + A_var \n",
    "\n",
    "def extract_data(filename, get_cols=False):\n",
    "    with h5py.File(filename, 'r') as infile:\n",
    "        # train data\n",
    "        W_dev = np.array(infile.get('W_dev'))          \n",
    "        X_s_dev = np.array(infile.get('X_s_dev'))       \n",
    "        X_v_dev = np.array(infile.get('X_v_dev'))      \n",
    "        T_dev = np.array(infile.get('T_dev'))       \n",
    "        y_train = np.array(infile.get('Y_dev'))           \n",
    "        A_dev = np.array(infile.get('A_dev')) \n",
    "        x_train = np.concatenate((W_dev, X_s_dev, X_v_dev, T_dev, A_dev), axis=1)\n",
    "\n",
    "        # test data\n",
    "        W_test = np.array(infile.get('W_test'))        \n",
    "        X_s_test = np.array(infile.get('X_s_test'))     \n",
    "        X_v_test = np.array(infile.get('X_v_test'))      \n",
    "        T_test = np.array(infile.get('T_test'))         \n",
    "        y_test = np.array(infile.get('Y_test'))         \n",
    "        A_test = np.array(infile.get('A_test')) \n",
    "        x_test = np.concatenate((W_test, X_s_test, X_v_test, T_test, A_test), axis=1)\n",
    "\n",
    "    # concat data together for EDA\n",
    "    #X = np.concatenate((x_train, x_test), axis=0)\n",
    "    #y = np.concatenate((y_train, y_test), axis=0)\n",
    "    \n",
    "    # possibly extract columns\n",
    "    cols = None\n",
    "    if get_cols:\n",
    "        cols = extract_columns(filename)\n",
    "\n",
    "    return {\"data\": (x_train, x_test, y_train, y_test), \"columns\": cols}\n",
    "\n",
    "def read_data(files):\n",
    "    data = extract_data(files[0], get_cols=True)\n",
    "    columns = data[\"columns\"]\n",
    "    X, y = data[\"data\"]\n",
    "    for filename in files[1:]:\n",
    "        X_temp, y_temp = extract_data(filename)[\"data\"]\n",
    "        X, y = np.concatenate((X, X_temp), axis=0), np.concatenate((y, y_temp), axis=0)\n",
    "    return X, y, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb7e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(files, output_file, id_cols, chunk_size=10000):\n",
    "    train_file = output_file + \"_train.h5\"\n",
    "    test_file = output_file + \"_test.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    train_size, test_size = 0, 0\n",
    "\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        data = extract_data(filename, get_cols=i==0)  # Read one file at a time\n",
    "        X_train, X_test, y_train, y_test = data[\"data\"]\n",
    "        y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "\n",
    "        if i == 0:\n",
    "            columns = data[\"columns\"]\n",
    "            id_idxs = [columns.index(col) for col in id_cols]\n",
    "            # make starting datasets\n",
    "            h5_train.create_dataset(\"X\", shape=(0, X_train.shape[1]), maxshape=(None, X_train.shape[1]), dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_train.shape[1]))\n",
    "            h5_train.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", chunks=(chunk_size,))\n",
    "\n",
    "            h5_test.create_dataset(\"X\", shape=(0, X_test.shape[1]), maxshape=(None, X_test.shape[1]), dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_test.shape[1]))\n",
    "            h5_test.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", chunks=(chunk_size,))\n",
    "\n",
    "            h5_train.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "            h5_test.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "\n",
    "\n",
    "\n",
    "        # append data incrementally to avoid memory issues\n",
    "        # fix shapes\n",
    "        h5_train[\"X\"].resize((train_size + X_train.shape[0]), axis=0)\n",
    "        h5_train[\"X\"][train_size:] = X_train\n",
    "        h5_train[\"y\"].resize((train_size + y_train.shape[0]), axis=0)\n",
    "        h5_train[\"y\"][train_size:] = y_train\n",
    "\n",
    "        h5_test[\"X\"].resize((test_size + X_test.shape[0]), axis=0)\n",
    "        h5_test[\"X\"][test_size:] = X_test\n",
    "        h5_test[\"y\"].resize((test_size + y_test.shape[0]), axis=0)\n",
    "        h5_test[\"y\"][test_size:] = y_test\n",
    "\n",
    "        train_size += X_train.shape[0]\n",
    "        test_size += X_test.shape[0]\n",
    "\n",
    "    # cleanup\n",
    "    h5_train.close()\n",
    "    h5_test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d559cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N-CMAPSS_DS01-005.h5 (1/9)...\n",
      "Processing N-CMAPSS_DS02-006.h5 (2/9)...\n",
      "Processing N-CMAPSS_DS03-012.h5 (3/9)...\n",
      "Processing N-CMAPSS_DS04.h5 (4/9)...\n",
      "Processing N-CMAPSS_DS05.h5 (5/9)...\n",
      "Processing N-CMAPSS_DS06.h5 (6/9)...\n",
      "Processing N-CMAPSS_DS07.h5 (7/9)...\n",
      "Processing N-CMAPSS_DS08a-009.h5 (8/9)...\n",
      "Processing N-CMAPSS_DS08c-008.h5 (9/9)...\n"
     ]
    }
   ],
   "source": [
    "merge_files(files, \"engine_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901f382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(files, output_file, id_cols, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Merges multiple HDF5 files into a single train/test HDF5 dataset and creates an index.csv file.\n",
    "\n",
    "    Parameters:\n",
    "        files (list): List of HDF5 file paths to merge.\n",
    "        output_file (str): Output HDF5 file path (without extension).\n",
    "        id_cols (list): List of column names identifying a time series (e.g., [\"unit\", \"cycle\"]).\n",
    "        chunk_size (int): Number of rows to process in each batch (to prevent memory issues).\n",
    "    \"\"\"\n",
    "    train_file = output_file + \"_train.h5\"\n",
    "    test_file = output_file + \"_test.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    train_size, test_size = 0, 0\n",
    "    index_data = []  # Store (unit, cycle, start, stop, dataset_type)\n",
    "\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        data = extract_data(filename, get_cols=(i == 0))  # Read one file at a time\n",
    "        X_train, X_test, y_train, y_test = data[\"data\"]\n",
    "        y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "\n",
    "        if i == 0:\n",
    "            columns = data[\"columns\"]\n",
    "\n",
    "            # Create datasets\n",
    "            h5_train.create_dataset(\"X\", shape=(0, X_train.shape[1]), maxshape=(None, X_train.shape[1]), \n",
    "                                    dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_train.shape[1]))\n",
    "            h5_train.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", \n",
    "                                    chunks=(chunk_size,))\n",
    "\n",
    "            h5_test.create_dataset(\"X\", shape=(0, X_test.shape[1]), maxshape=(None, X_test.shape[1]), \n",
    "                                   dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_test.shape[1]))\n",
    "            h5_test.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", \n",
    "                                   chunks=(chunk_size,))\n",
    "\n",
    "            h5_train.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "            h5_test.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "\n",
    "        # ** Extract indices of ID columns **\n",
    "        id_idxs = [columns.index(col) for col in id_cols]\n",
    "\n",
    "        # ** Extract unique (unit, cycle) time series for train and test sets **\n",
    "        for dataset_type, X_data, base_index in [(\"train\", X_train, train_size), (\"test\", X_test, test_size)]:\n",
    "            if X_data.shape[0] == 0:\n",
    "                continue  # Skip if there's no data\n",
    "\n",
    "            # Load only unit and cycle columns\n",
    "            id_values = X_data[:, id_idxs]\n",
    "            id_df = pd.DataFrame(id_values, columns=id_cols)\n",
    "\n",
    "            # Get start and stop indices for each unique (unit, cycle)\n",
    "            grouped = id_df.groupby(id_cols).apply(lambda df: (df.index.min(), df.index.max()))\n",
    "            for (unit, cycle), (start, stop) in grouped.items():\n",
    "                index_data.append([unit, cycle, start + base_index, stop + base_index, dataset_type])\n",
    "\n",
    "        # ** Append data incrementally to avoid memory issues **\n",
    "        h5_train[\"X\"].resize((train_size + X_train.shape[0]), axis=0)\n",
    "        h5_train[\"X\"][train_size:] = X_train\n",
    "        h5_train[\"y\"].resize((train_size + y_train.shape[0]), axis=0)\n",
    "        h5_train[\"y\"][train_size:] = y_train\n",
    "\n",
    "        h5_test[\"X\"].resize((test_size + X_test.shape[0]), axis=0)\n",
    "        h5_test[\"X\"][test_size:] = X_test\n",
    "        h5_test[\"y\"].resize((test_size + y_test.shape[0]), axis=0)\n",
    "        h5_test[\"y\"][test_size:] = y_test\n",
    "\n",
    "        train_size += X_train.shape[0]\n",
    "        test_size += X_test.shape[0]\n",
    "\n",
    "    # ** Save the index file as CSV **\n",
    "    index_df = pd.DataFrame(index_data, columns=[\"unit\", \"cycle\", \"start_idx\", \"stop_idx\", \"dataset\"])\n",
    "    index_df.to_csv(output_file + \"_index.csv\", index=False)\n",
    "    print(f\"✅ Index file saved: {output_file}_index.csv\")\n",
    "\n",
    "    # Cleanup\n",
    "    h5_train.close()\n",
    "    h5_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84914e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N-CMAPSS_DS01-005.h5 (1/9)...\n",
      "Processing N-CMAPSS_DS02-006.h5 (2/9)...\n",
      "Processing N-CMAPSS_DS03-012.h5 (3/9)...\n",
      "Processing N-CMAPSS_DS04.h5 (4/9)...\n",
      "Processing N-CMAPSS_DS05.h5 (5/9)...\n",
      "Processing N-CMAPSS_DS06.h5 (6/9)...\n",
      "Processing N-CMAPSS_DS07.h5 (7/9)...\n",
      "Processing N-CMAPSS_DS08a-009.h5 (8/9)...\n",
      "Processing N-CMAPSS_DS08c-008.h5 (9/9)...\n",
      "✅ Index file saved: engine_index.csv\n"
     ]
    }
   ],
   "source": [
    "merge_files(files, \"engine\", id_cols=[\"unit\", \"cycle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cd7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def merge_files_timewindows(\n",
    "    files,\n",
    "    output_file,\n",
    "    id_cols,\n",
    "    window_size=50,\n",
    "    overlap=10,\n",
    "    chunk_size=10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Merges multiple HDF5 files into train/test windowed HDF5 datasets.\n",
    "    The output files contain shape (num_windows, window_size, num_features).\n",
    "\n",
    "    Parameters:\n",
    "        files (list of str): List of HDF5 file paths to merge.\n",
    "        output_file (str): Basename for output HDF5 files (no \".h5\" extension).\n",
    "                           This will produce \"{output_file}_train_windows.h5\" and\n",
    "                           \"{output_file}_test_windows.h5\".\n",
    "        id_cols (list of str): Column names identifying each time series, e.g. [\"unit\", \"cycle\"].\n",
    "        window_size (int): Size of each time window (# of observations).\n",
    "        overlap (int): Number of overlapping rows between consecutive windows.\n",
    "        chunk_size (int): Used when creating chunked HDF5 datasets. Does NOT\n",
    "                          control reading chunk size from the input files,\n",
    "                          but sets chunk shape in the output HDF5.\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1. Prepare output HDF5 files for train and test windows\n",
    "    # ----------------------------------------------------------------------\n",
    "    train_file = output_file + \"_train_windows.h5\"\n",
    "    test_file = output_file + \"_test_windows.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    # We'll create empty placeholders; we don't know the final # of windows yet.\n",
    "    X_train_dset = None\n",
    "    y_train_dset = None\n",
    "    X_test_dset = None\n",
    "    y_test_dset = None\n",
    "\n",
    "    train_count = 0  # How many train windows we've appended so far\n",
    "    test_count = 0   # How many test windows we've appended so far\n",
    "\n",
    "    # Step size for time windows (controls overlap)\n",
    "    step_size = window_size - overlap\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2. Helper function for time-window generation on a single ID group\n",
    "    # ----------------------------------------------------------------------\n",
    "    def create_time_windows(X_sub, y_sub):\n",
    "        \"\"\"\n",
    "        Given the X and y arrays for a single ID group (time-series),\n",
    "        generate all windows with shape (num_windows, window_size, num_features).\n",
    "        For the label, we take y at the *end* of each window (common RUL approach).\n",
    "        \"\"\"\n",
    "        windows_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        n_samples = X_sub.shape[0]\n",
    "        # Go up to n_samples - window_size + 1\n",
    "        for start_idx in range(0, n_samples - window_size + 1, step_size):\n",
    "            end_idx = start_idx + window_size\n",
    "            windows_list.append(X_sub[start_idx:end_idx])\n",
    "            # Label often taken from the last row in the window\n",
    "            labels_list.append(y_sub[end_idx - 1])\n",
    "\n",
    "        if len(windows_list) == 0:\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        # Stack them into final arrays\n",
    "        Xw = np.stack(windows_list, axis=0)  # shape: (num_windows, window_size, num_features)\n",
    "        yw = np.array(labels_list, dtype=Xw.dtype)  # shape: (num_windows,)\n",
    "        return Xw, yw\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3. Process each file in 'files'\n",
    "    #    We assume you have a function `extract_data(h5_path, get_cols=False)`\n",
    "    #    that returns { \"data\": (X_train, X_test, y_train, y_test), \"columns\": [...] }\n",
    "    # ----------------------------------------------------------------------\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3a. Extract the data from the source file\n",
    "        # ----------------------------------------------------------------------\n",
    "        # You need to implement or already have `extract_data`; it typically does:\n",
    "        #   X_train, X_test, y_train, y_test = ...\n",
    "        #   columns = [...]\n",
    "        #   return { \"data\": (X_train, X_test, y_train, y_test), \"columns\": columns }\n",
    "        data_dict = extract_data(filename, get_cols=True)\n",
    "        X_train_raw, X_test_raw, y_train_raw, y_test_raw = data_dict[\"data\"]\n",
    "        y_train_raw = y_train_raw.squeeze()\n",
    "        y_test_raw = y_test_raw.squeeze()\n",
    "        columns = data_dict[\"columns\"]\n",
    "        print(columns)\n",
    "\n",
    "        # If this is the first file, create the output datasets\n",
    "        if i == 0:\n",
    "            # We remove the ID columns + target from the \"feature\" set\n",
    "            id_idxs = [columns.index(col) for col in id_cols]\n",
    "            target_idx = columns.index(\"RUL\") if \"RUL\" in columns else None\n",
    "\n",
    "            feature_cols = [\n",
    "                c for c in range(len(columns))\n",
    "                if (c not in id_idxs) and (c != target_idx)\n",
    "            ]\n",
    "            num_features = len(feature_cols)\n",
    "\n",
    "            # Create placeholders for train/test window sets\n",
    "            X_train_dset = h5_train.create_dataset(\n",
    "                \"X\", shape=(0, window_size, num_features),\n",
    "                maxshape=(None, window_size, num_features),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size, window_size, num_features)\n",
    "            )\n",
    "            y_train_dset = h5_train.create_dataset(\n",
    "                \"y\", shape=(0,),\n",
    "                maxshape=(None,),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size,)\n",
    "            )\n",
    "\n",
    "            X_test_dset = h5_test.create_dataset(\n",
    "                \"X\", shape=(0, window_size, num_features),\n",
    "                maxshape=(None, window_size, num_features),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size, window_size, num_features)\n",
    "            )\n",
    "            y_test_dset = h5_test.create_dataset(\n",
    "                \"y\", shape=(0,),\n",
    "                maxshape=(None,),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size,)\n",
    "            )\n",
    "\n",
    "            # Store attribute with feature column names if you want\n",
    "            # (excluding ID + target columns)\n",
    "            kept_column_names = [columns[c] for c in feature_cols]\n",
    "            h5_train.attrs[\"columns\"] = np.array(kept_column_names, dtype=\"S\")\n",
    "            h5_test.attrs[\"columns\"] = np.array(kept_column_names, dtype=\"S\")\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3b. Group the train set by ID columns, generate windows, append them\n",
    "        # ----------------------------------------------------------------------\n",
    "        if X_train_raw.shape[0] > 0:\n",
    "            # Convert to DataFrame for grouping\n",
    "            df_train = pd.DataFrame(X_train_raw, columns=columns)\n",
    "            df_train[\"RUL\"] = y_train_raw\n",
    "\n",
    "            # For each unique ID, build windows\n",
    "            group_df_train = df_train.groupby(id_cols, sort=False)\n",
    "            for _, sub_df in group_df_train:\n",
    "                # Separate features vs. label\n",
    "                # - We'll keep only the relevant feature columns\n",
    "                # - We'll keep label from the 'RUL' column\n",
    "                sub_X = sub_df.iloc[:, feature_cols].values  # shape (n, num_features)\n",
    "                sub_y = sub_df[\"RUL\"].values\n",
    "\n",
    "                # Generate windows\n",
    "                Xw, yw = create_time_windows(sub_X, sub_y)\n",
    "                if Xw.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                # Append to HDF5\n",
    "                n_new = Xw.shape[0]\n",
    "                X_train_dset.resize(X_train_dset.shape[0] + n_new, axis=0)\n",
    "                y_train_dset.resize(y_train_dset.shape[0] + n_new, axis=0)\n",
    "\n",
    "                X_train_dset[-n_new:] = Xw\n",
    "                y_train_dset[-n_new:] = yw\n",
    "                train_count += n_new\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3c. Same logic for test set\n",
    "        # ----------------------------------------------------------------------\n",
    "        if X_test_raw.shape[0] > 0:\n",
    "            # Convert to DataFrame for grouping\n",
    "            df_test = pd.DataFrame(X_test_raw, columns=columns)\n",
    "            df_test[\"RUL\"] = y_test_raw\n",
    "\n",
    "            group_df_test = df_test.groupby(id_cols, sort=False)\n",
    "            for _, sub_df in group_df_test:\n",
    "                sub_X = sub_df.iloc[:, feature_cols].values\n",
    "                sub_y = sub_df[\"RUL\"].values\n",
    "\n",
    "                Xw, yw = create_time_windows(sub_X, sub_y)\n",
    "                if Xw.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                n_new = Xw.shape[0]\n",
    "                X_test_dset.resize(X_test_dset.shape[0] + n_new, axis=0)\n",
    "                y_test_dset.resize(y_test_dset.shape[0] + n_new, axis=0)\n",
    "\n",
    "                X_test_dset[-n_new:] = Xw\n",
    "                y_test_dset[-n_new:] = yw\n",
    "                test_count += n_new\n",
    "\n",
    "        print(f\"  ...done. Train windows so far: {train_count}, Test windows so far: {test_count}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4. Cleanup and Close\n",
    "    # ----------------------------------------------------------------------\n",
    "    h5_train.close()\n",
    "    h5_test.close()\n",
    "    print(f\"✅ Train windows file saved: {train_file}  (total windows: {train_count})\")\n",
    "    print(f\"✅ Test windows file saved: {test_file}   (total windows: {test_count})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55755d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../N-CMAPSS_DS01-005.h5 (1/9)...\n",
      "['alt', 'Mach', 'TRA', 'T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', 'T40', 'P30', 'P45', 'W21', 'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'SmFan', 'SmLPC', 'SmHPC', 'phi', 'fan_eff_mod', 'fan_flow_mod', 'LPC_eff_mod', 'LPC_flow_mod', 'HPC_eff_mod', 'HPC_flow_mod', 'HPT_eff_mod', 'HPT_flow_mod', 'LPT_eff_mod', 'LPT_flow_mod', 'unit', 'cycle', 'Fc', 'hs']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerge_files_timewindows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mengine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# -> produces \"merged_engine_train_windows.h5\" and \"merged_engine_test_windows.h5\"\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcycle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\u001b[38;5;66;43;03m# windows will have step_size=45\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# chunk shape for output dataset\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmerge_files_timewindows\u001b[0;34m(files, output_file, id_cols, window_size, overlap, chunk_size)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# For each unique ID, build windows\u001b[39;00m\n\u001b[1;32m    157\u001b[0m group_df_train \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mgroupby(id_cols, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, sub_df \u001b[38;5;129;01min\u001b[39;00m group_df_train:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Separate features vs. label\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# - We'll keep only the relevant feature columns\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# - We'll keep label from the 'RUL' column\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     sub_X \u001b[38;5;241m=\u001b[39m sub_df\u001b[38;5;241m.\u001b[39miloc[:, feature_cols]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# shape (n, num_features)\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     sub_y \u001b[38;5;241m=\u001b[39m sub_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/ops.py:708\u001b[0m, in \u001b[0;36mBaseGrouper.get_iterator\u001b[0;34m(self, data, axis)\u001b[0m\n\u001b[1;32m    706\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(data, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    707\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_keys_seq\n\u001b[0;32m--> 708\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, splitter):\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m key, group\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/ops.py:1223\u001b[0m, in \u001b[0;36mDataSplitter.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1223\u001b[0m     sdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msorted_data\u001b[49m\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1226\u001b[0m         \u001b[38;5;66;03m# we are inside a generator, rather than raise StopIteration\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m         \u001b[38;5;66;03m# we merely return signal the end\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/ops.py:1237\u001b[0m, in \u001b[0;36mDataSplitter.sorted_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msorted_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m-> 1237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sort_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:3703\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3699\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_take((), kwargs)\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m-> 3703\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   3705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py:900\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    897\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    899\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py:692\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    685\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    686\u001b[0m         indexer,\n\u001b[1;32m    687\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    688\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[1;32m    689\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[1;32m    690\u001b[0m     )\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 692\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    693\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    694\u001b[0m             indexer,\n\u001b[1;32m    695\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    696\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    697\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m    698\u001b[0m             ),\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    701\u001b[0m     ]\n\u001b[1;32m    703\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    704\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py:693\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    685\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    686\u001b[0m         indexer,\n\u001b[1;32m    687\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    688\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[1;32m    689\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[1;32m    690\u001b[0m     )\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 693\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    701\u001b[0m     ]\n\u001b[1;32m    703\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    704\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py:1139\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m#  this assertion\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m new_mgr_locs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/array_algos/take.py:163\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    158\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    160\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    161\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    162\u001b[0m )\n\u001b[0;32m--> 163\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[1;32m    166\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merge_files_timewindows(\n",
    "    files=files,\n",
    "    output_file=\"engine\",         # -> produces \"merged_engine_train_windows.h5\" and \"merged_engine_test_windows.h5\"\n",
    "    id_cols=[\"unit\", \"cycle\"],\n",
    "    window_size=50,\n",
    "    overlap=5,                           # windows will have step_size=45\n",
    "    chunk_size=1000                      # chunk shape for output dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13402fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_hdf5(filename=\"data.h5\"):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        X = f[\"X\"][:]\n",
    "        y = f[\"y\"][:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcafa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_from_hdf5(filename=\"engine_train_windows.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b917db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980793, 50, 44)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ef7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def merge_files_timewindows(\n",
    "    files,\n",
    "    output_file,\n",
    "    id_cols,\n",
    "    window_size=50,\n",
    "    overlap=10,\n",
    "    chunk_size=10000,\n",
    "    metadata_cols=None,\n",
    "    keep_metadata=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Merges multiple HDF5 files into train/test windowed HDF5 datasets,\n",
    "    storing X, y, and (optionally) additional metadata in separate datasets.\n",
    "\n",
    "    The output files each contain:\n",
    "        - \"X\":   shape (num_windows, window_size, num_features)\n",
    "        - \"y\":   shape (num_windows,)\n",
    "        - \"meta\": shape (num_windows, window_size, num_metadata_features)\n",
    "                  (only if keep_metadata=True and metadata_cols is non-empty)\n",
    "    \n",
    "    Parameters:\n",
    "        files (list of str): List of HDF5 file paths to merge.\n",
    "        output_file (str): Basename for output HDF5 files (no \".h5\" extension).\n",
    "                           This will produce \"{output_file}_train_windows.h5\" \n",
    "                           and \"{output_file}_test_windows.h5\".\n",
    "        id_cols (list of str): Column names identifying each time series \n",
    "                               (e.g. [\"unit\", \"cycle\"]).\n",
    "        window_size (int): Size of each time window (# of observations).\n",
    "        overlap (int): Number of overlapping rows between consecutive windows.\n",
    "        chunk_size (int): Chunk shape for storing the data in HDF5 \n",
    "                          (affects I/O performance).\n",
    "        metadata_cols (list of str): Column names to store as metadata. \n",
    "                                     These are stored in a separate dataset \n",
    "                                     and NOT fed to the model (not in X).\n",
    "        keep_metadata (bool): If False, metadata is fully discarded (no \"meta\" \n",
    "                              dataset). This saves memory/disk if you don't \n",
    "                              need metadata.\n",
    "\n",
    "    Example usage:\n",
    "        merge_files_timewindows(\n",
    "            files=[\"engine1.h5\", \"engine2.h5\"],\n",
    "            output_file=\"merged_engine\",\n",
    "            id_cols=[\"unit\", \"cycle\"],\n",
    "            window_size=50,\n",
    "            overlap=5,\n",
    "            chunk_size=2000,\n",
    "            metadata_cols=[\"unit\", \"cycle\", \"health_status\"],\n",
    "            keep_metadata=True\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1. Prepare output HDF5 files for train and test windows\n",
    "    # ----------------------------------------------------------------------\n",
    "    train_file = output_file + \"_train_windows.h5\"\n",
    "    test_file = output_file + \"_test_windows.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    X_train_dset = None\n",
    "    y_train_dset = None\n",
    "    M_train_dset = None  # for metadata if keep_metadata=True\n",
    "    X_test_dset = None\n",
    "    y_test_dset = None\n",
    "    M_test_dset = None   # for metadata if keep_metadata=True\n",
    "\n",
    "    train_count = 0  # number of train windows appended so far\n",
    "    test_count = 0   # number of test windows appended so far\n",
    "\n",
    "    # Step size for time windows (controls overlap)\n",
    "    step_size = window_size - overlap\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2. Time-window generation function\n",
    "    # ----------------------------------------------------------------------\n",
    "    def create_time_windows(X_sub, y_sub=None):\n",
    "        \"\"\"\n",
    "        Generates overlapping time windows from X_sub (and optionally from y_sub).\n",
    "\n",
    "        - X_sub: shape (n_samples, num_features)\n",
    "        - y_sub: shape (n_samples,) or None\n",
    "\n",
    "        Returns:\n",
    "            Xw: shape (num_windows, window_size, num_features)\n",
    "            yw: shape (num_windows,) if y_sub is given; otherwise None\n",
    "        \"\"\"\n",
    "        windows_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        n_samples = X_sub.shape[0]\n",
    "        # Generate windows with the specified step size\n",
    "        for start_idx in range(0, n_samples - window_size + 1, step_size):\n",
    "            end_idx = start_idx + window_size\n",
    "            windows_list.append(X_sub[start_idx:end_idx])\n",
    "            if y_sub is not None:\n",
    "                # Common RUL approach: label from the last row of the window\n",
    "                labels_list.append(y_sub[end_idx - 1])\n",
    "\n",
    "        if len(windows_list) == 0:\n",
    "            return np.array([]), (np.array([]) if y_sub is not None else None)\n",
    "\n",
    "        # Stack them into final arrays\n",
    "        Xw = np.stack(windows_list, axis=0)  # shape: (num_windows, window_size, num_features)\n",
    "        if y_sub is not None:\n",
    "            yw = np.array(labels_list, dtype=Xw.dtype)  # shape: (num_windows,)\n",
    "            return Xw, yw\n",
    "        else:\n",
    "            return Xw, None\n",
    "\n",
    "    if metadata_cols is None:\n",
    "        metadata_cols = []\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3. Process each file in 'files'\n",
    "    # ----------------------------------------------------------------------\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        # This function should return:\n",
    "        # { \"data\": (X_train, X_test, y_train, y_test), \"columns\": [...] }\n",
    "        data_dict = extract_data(filename, get_cols=True)\n",
    "        X_train_raw, X_test_raw, y_train_raw, y_test_raw = data_dict[\"data\"]\n",
    "        y_train_raw = y_train_raw.squeeze()\n",
    "        y_test_raw = y_test_raw.squeeze()\n",
    "        columns = data_dict[\"columns\"]\n",
    "        print(\"Columns:\", columns)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3a. Create the output datasets on the FIRST iteration only\n",
    "        # ------------------------------------------------------------------\n",
    "        if i == 0:\n",
    "            # Identify which columns go in X, which go in metadata\n",
    "            id_idxs = [columns.index(col) for col in id_cols]\n",
    "            target_idx = columns.index(\"RUL\") if \"RUL\" in columns else None\n",
    "            meta_idxs = [columns.index(col) for col in metadata_cols if col in columns]\n",
    "\n",
    "            # The X feature set excludes id_cols, target_col, and metadata_cols\n",
    "            feature_cols = [\n",
    "                c for c in range(len(columns))\n",
    "                if (c not in id_idxs) and (c != target_idx) and (c not in meta_idxs)\n",
    "            ]\n",
    "            print([columns[i] for i in feature_cols])\n",
    "            num_features = len(feature_cols)\n",
    "            num_meta = len(meta_idxs) if keep_metadata else 0\n",
    "\n",
    "            # Create placeholders for train/test window sets: X, y (always),\n",
    "            # plus meta (only if keep_metadata is True).\n",
    "            X_train_dset = h5_train.create_dataset(\n",
    "                \"X\", \n",
    "                shape=(0, window_size, num_features),\n",
    "                maxshape=(None, window_size, num_features),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size, window_size, num_features)\n",
    "            )\n",
    "            y_train_dset = h5_train.create_dataset(\n",
    "                \"y\", \n",
    "                shape=(0,),\n",
    "                maxshape=(None,),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size,)\n",
    "            )\n",
    "            if keep_metadata and num_meta > 0:\n",
    "                M_train_dset = h5_train.create_dataset(\n",
    "                    \"meta\",\n",
    "                    shape=(0, window_size, num_meta),\n",
    "                    maxshape=(None, window_size, num_meta),\n",
    "                    dtype='float32',\n",
    "                    compression=\"gzip\",\n",
    "                    chunks=(chunk_size, window_size, num_meta)\n",
    "                )\n",
    "\n",
    "            X_test_dset = h5_test.create_dataset(\n",
    "                \"X\", \n",
    "                shape=(0, window_size, num_features),\n",
    "                maxshape=(None, window_size, num_features),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size, window_size, num_features)\n",
    "            )\n",
    "            y_test_dset = h5_test.create_dataset(\n",
    "                \"y\", \n",
    "                shape=(0,),\n",
    "                maxshape=(None,),\n",
    "                dtype='float32',\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size,)\n",
    "            )\n",
    "            if keep_metadata and num_meta > 0:\n",
    "                M_test_dset = h5_test.create_dataset(\n",
    "                    \"meta\",\n",
    "                    shape=(0, window_size, num_meta),\n",
    "                    maxshape=(None, window_size, num_meta),\n",
    "                    dtype='float32',\n",
    "                    compression=\"gzip\",\n",
    "                    chunks=(chunk_size, window_size, num_meta)\n",
    "                )\n",
    "\n",
    "            # Store feature/metadata columns as attributes for reference\n",
    "            kept_feature_colnames = [columns[c] for c in feature_cols]\n",
    "            h5_train.attrs[\"feature_columns\"] = np.array(kept_feature_colnames, dtype=\"S\")\n",
    "            h5_test.attrs[\"feature_columns\"] = np.array(kept_feature_colnames, dtype=\"S\")\n",
    "\n",
    "            if keep_metadata and num_meta > 0:\n",
    "                kept_meta_colnames = [columns[c] for c in meta_idxs]\n",
    "                h5_train.attrs[\"metadata_columns\"] = np.array(kept_meta_colnames, dtype=\"S\")\n",
    "                h5_test.attrs[\"metadata_columns\"] = np.array(kept_meta_colnames, dtype=\"S\")\n",
    "            else:\n",
    "                # Store empty for clarity\n",
    "                h5_train.attrs[\"metadata_columns\"] = np.array([], dtype=\"S\")\n",
    "                h5_test.attrs[\"metadata_columns\"] = np.array([], dtype=\"S\")\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3b. Build windows for the TRAIN set\n",
    "        # ----------------------------------------------------------------------\n",
    "        if X_train_raw.shape[0] > 0:\n",
    "            df_train = pd.DataFrame(X_train_raw, columns=columns)\n",
    "            df_train[\"RUL\"] = y_train_raw\n",
    "\n",
    "            # Group by the ID columns to ensure no window spans multiple IDs\n",
    "            group_df_train = df_train.groupby(id_cols, sort=False)\n",
    "            for _, sub_df in group_df_train:\n",
    "                # Create X windows\n",
    "                sub_X = sub_df.iloc[:, feature_cols].values  # shape (n, num_features)\n",
    "                sub_y = sub_df[\"RUL\"].values\n",
    "                Xw, yw = create_time_windows(sub_X, sub_y)\n",
    "\n",
    "                if Xw.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                # If we're keeping metadata, build meta windows\n",
    "                if keep_metadata and num_meta > 0:\n",
    "                    sub_M = sub_df.iloc[:, meta_idxs].values\n",
    "                    Mw, _ = create_time_windows(sub_M)\n",
    "                else:\n",
    "                    Mw = np.array([])\n",
    "\n",
    "                # Append to HDF5\n",
    "                n_new = Xw.shape[0]\n",
    "                # Expand X\n",
    "                X_train_dset.resize(X_train_dset.shape[0] + n_new, axis=0)\n",
    "                X_train_dset[-n_new:] = Xw\n",
    "\n",
    "                # Expand y\n",
    "                y_train_dset.resize(y_train_dset.shape[0] + n_new, axis=0)\n",
    "                y_train_dset[-n_new:] = yw\n",
    "\n",
    "                # Expand meta (if we have it)\n",
    "                if keep_metadata and num_meta > 0 and Mw.size > 0:\n",
    "                    M_train_dset.resize(M_train_dset.shape[0] + n_new, axis=0)\n",
    "                    M_train_dset[-n_new:] = Mw\n",
    "\n",
    "                train_count += n_new\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 3c. Build windows for the TEST set\n",
    "        # ----------------------------------------------------------------------\n",
    "        if X_test_raw.shape[0] > 0:\n",
    "            df_test = pd.DataFrame(X_test_raw, columns=columns)\n",
    "            df_test[\"RUL\"] = y_test_raw\n",
    "\n",
    "            group_df_test = df_test.groupby(id_cols, sort=False)\n",
    "            for _, sub_df in group_df_test:\n",
    "                sub_X = sub_df.iloc[:, feature_cols].values\n",
    "                sub_y = sub_df[\"RUL\"].values\n",
    "                Xw, yw = create_time_windows(sub_X, sub_y)\n",
    "\n",
    "                if Xw.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                if keep_metadata and num_meta > 0:\n",
    "                    sub_M = sub_df.iloc[:, meta_idxs].values\n",
    "                    Mw, _ = create_time_windows(sub_M)\n",
    "                else:\n",
    "                    Mw = np.array([])\n",
    "\n",
    "                # Append to HDF5\n",
    "                n_new = Xw.shape[0]\n",
    "                # Expand X\n",
    "                X_test_dset.resize(X_test_dset.shape[0] + n_new, axis=0)\n",
    "                X_test_dset[-n_new:] = Xw\n",
    "\n",
    "                # Expand y\n",
    "                y_test_dset.resize(y_test_dset.shape[0] + n_new, axis=0)\n",
    "                y_test_dset[-n_new:] = yw\n",
    "\n",
    "                # Expand meta (if we have it)\n",
    "                if keep_metadata and num_meta > 0 and Mw.size > 0:\n",
    "                    M_test_dset.resize(M_test_dset.shape[0] + n_new, axis=0)\n",
    "                    M_test_dset[-n_new:] = Mw\n",
    "\n",
    "                test_count += n_new\n",
    "\n",
    "        print(f\"  ...done. Train windows so far: {train_count}, Test windows so far: {test_count}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4. Cleanup and Close\n",
    "    # ----------------------------------------------------------------------\n",
    "    h5_train.close()\n",
    "    h5_test.close()\n",
    "    print(f\"✅ Train windows file saved: {train_file}  (total windows: {train_count})\")\n",
    "    print(f\"✅ Test windows file saved: {test_file}   (total windows: {test_count})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875039b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../data/N-CMAPSS_DS01-005.h5 (1/9)...\n",
      "Columns: ['alt', 'Mach', 'TRA', 'T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', 'T40', 'P30', 'P45', 'W21', 'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'SmFan', 'SmLPC', 'SmHPC', 'phi', 'fan_eff_mod', 'fan_flow_mod', 'LPC_eff_mod', 'LPC_flow_mod', 'HPC_eff_mod', 'HPC_flow_mod', 'HPT_eff_mod', 'HPT_flow_mod', 'LPT_eff_mod', 'LPT_flow_mod', 'unit', 'cycle', 'Fc', 'hs']\n",
      "['alt', 'Mach', 'TRA', 'T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', 'T40', 'P30', 'P45', 'W21', 'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'SmFan', 'SmLPC', 'SmHPC', 'phi', 'fan_eff_mod', 'fan_flow_mod', 'LPC_eff_mod', 'LPC_flow_mod', 'HPC_eff_mod', 'HPC_flow_mod', 'HPT_eff_mod', 'HPT_flow_mod', 'LPT_eff_mod', 'LPT_flow_mod', 'Fc']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerge_files_timewindows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mengine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# -> produces \"merged_engine_train_windows.h5\" and \"merged_engine_test_windows.h5\"\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcycle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\u001b[38;5;66;43;03m# windows will have step_size=45\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# chunk shape for output dataset\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mmerge_files_timewindows\u001b[0;34m(files, output_file, id_cols, window_size, overlap, chunk_size, metadata_cols, keep_metadata)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Expand X\u001b[39;00m\n\u001b[1;32m    246\u001b[0m X_train_dset\u001b[38;5;241m.\u001b[39mresize(X_train_dset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m n_new, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m X_train_dset[\u001b[38;5;241m-\u001b[39mn_new:] \u001b[38;5;241m=\u001b[39m Xw\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Expand y\u001b[39;00m\n\u001b[1;32m    250\u001b[0m y_train_dset\u001b[38;5;241m.\u001b[39mresize(y_train_dset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m n_new, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/dataset.py:708\u001b[0m, in \u001b[0;36mDataset.__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    706\u001b[0m mspace \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(mshape_pad, (h5s\u001b[38;5;241m.\u001b[39mUNLIMITED,)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(mshape_pad))\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fspace \u001b[38;5;129;01min\u001b[39;00m selection\u001b[38;5;241m.\u001b[39mbroadcast(mshape):\n\u001b[0;32m--> 708\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merge_files_timewindows(\n",
    "    files=files,\n",
    "    output_file=\"engine\",         # -> produces \"merged_engine_train_windows.h5\" and \"merged_engine_test_windows.h5\"\n",
    "    id_cols=[\"unit\", \"cycle\"],\n",
    "    metadata_cols=[\"unit\", \"hs\"],\n",
    "    window_size=50,\n",
    "    overlap=5,                           # windows will have step_size=45\n",
    "    chunk_size=1000,\n",
    "    keep_metadata=False # chunk shape for output dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a53e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_hdf5(\n",
    "    input_file,\n",
    "    output_file,\n",
    "    sample_size=500000,\n",
    "    seed=None,\n",
    "    dataset_chunk=(1000, None, None),  # (rows, window_size, num_features)\n",
    "    compression_opts=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a new HDF5 file as a random subsample of the original\n",
    "    \"X\", \"y\", and \"meta\" datasets, with configurable chunk shapes and compression.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    with h5py.File(input_file, \"r\") as f_in:\n",
    "        X_in = f_in[\"X\"]\n",
    "        num_windows, window_size, num_features = X_in.shape\n",
    "\n",
    "        # Adjust sample size\n",
    "        sample_size = min(sample_size, num_windows)\n",
    "\n",
    "        # Create random indices\n",
    "        all_indices = np.arange(num_windows)\n",
    "        np.random.shuffle(all_indices)\n",
    "        chosen_indices = np.sort(all_indices[:sample_size])\n",
    "\n",
    "        # Prepare optional data\n",
    "        y_in = f_in[\"y\"] if \"y\" in f_in else None\n",
    "        meta_in = f_in[\"meta\"] if \"meta\" in f_in else None\n",
    "\n",
    "        # --- Create output file ---\n",
    "        with h5py.File(output_file, \"w\") as f_out:\n",
    "            # Copy file-level attributes\n",
    "            for attr_key, attr_val in f_in.attrs.items():\n",
    "                f_out.attrs[attr_key] = attr_val\n",
    "\n",
    "            # Setup chunk shape\n",
    "            # e.g. (1000, window_size, num_features)\n",
    "            # If dataset_chunk is provided as e.g. (1000, None, None),\n",
    "            # we insert the actual window_size, num_features.\n",
    "            row_chunk = dataset_chunk[0] or 10000\n",
    "            win_chunk = dataset_chunk[1] or window_size\n",
    "            feat_chunk = dataset_chunk[2] or num_features\n",
    "\n",
    "            # Create X dataset\n",
    "            X_out = f_out.create_dataset(\n",
    "                \"X\",\n",
    "                shape=(sample_size, window_size, num_features),\n",
    "                dtype=X_in.dtype,\n",
    "                #compression=\"gzip\",            # or None if no compression\n",
    "                #compression_opts=compression_opts,\n",
    "                compression=None,\n",
    "                chunks=(row_chunk, win_chunk, feat_chunk)\n",
    "            )\n",
    "            for ak, av in X_in.attrs.items():\n",
    "                X_out.attrs[ak] = av\n",
    "\n",
    "            # Create y dataset if present\n",
    "            if y_in is not None:\n",
    "                y_out = f_out.create_dataset(\n",
    "                    \"y\",\n",
    "                    shape=(sample_size,),\n",
    "                    dtype=y_in.dtype,\n",
    "                    compression=None,\n",
    "                    #compression=\"gzip\",\n",
    "                    #compression_opts=compression_opts,\n",
    "                    chunks=(row_chunk,)\n",
    "                )\n",
    "                for ak, av in y_in.attrs.items():\n",
    "                    y_out.attrs[ak] = av\n",
    "            else:\n",
    "                y_out = None\n",
    "\n",
    "            # Create meta dataset if present\n",
    "            if meta_in is not None:\n",
    "                meta_out = f_out.create_dataset(\n",
    "                    \"meta\",\n",
    "                    shape=(sample_size,) + meta_in.shape[1:],\n",
    "                    dtype=meta_in.dtype,\n",
    "                    #compression=\"gzip\",\n",
    "                    #compression_opts=compression_opts,\n",
    "                    compression=None,\n",
    "                    chunks=(row_chunk,) + meta_in.shape[1:]\n",
    "                )\n",
    "                for ak, av in meta_in.attrs.items():\n",
    "                    meta_out.attrs[ak] = av\n",
    "            else:\n",
    "                meta_out = None\n",
    "\n",
    "            # --- Write in small batches ---\n",
    "            read_chunk_size = 20000\n",
    "            start = 0\n",
    "            while start < sample_size:\n",
    "                end = min(start + read_chunk_size, sample_size)\n",
    "                idx_chunk = chosen_indices[start:end]\n",
    "\n",
    "                X_chunk = X_in[idx_chunk]\n",
    "                X_out[start:end] = X_chunk\n",
    "\n",
    "                if y_out is not None:\n",
    "                    y_chunk = y_in[idx_chunk]\n",
    "                    y_out[start:end] = y_chunk\n",
    "\n",
    "                if meta_out is not None:\n",
    "                    meta_chunk = meta_in[idx_chunk]\n",
    "                    meta_out[start:end] = meta_chunk\n",
    "\n",
    "                start = end\n",
    "\n",
    "        print(f\"Sampled {sample_size} rows out of {num_windows} total.\")\n",
    "        print(f\"✅ Random sample saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19802da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 500000 rows out of 980793 total.\n",
      "✅ Random sample saved to ../../engine_train_windows_sample.h5\n"
     ]
    }
   ],
   "source": [
    "random_sample_hdf5(\n",
    "    input_file=\"../../engine_train_windows.h5\",\n",
    "    output_file=\"../../engine_train_windows_sample.h5\",\n",
    "    sample_size=500000,\n",
    "    seed=42,\n",
    "    compression_opts=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9711524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def min_max_scale_hdf5(\n",
    "    input_file,\n",
    "    output_file,\n",
    "    chunk_size=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads an HDF5 file with datasets \"X\", \"y\", and optionally \"meta\",\n",
    "    computes per-feature min/max for X, then writes out a new file\n",
    "    where X is min-max scaled to [0,1]. Other datasets (y, meta) are copied.\n",
    "\n",
    "    The feature-wise min/max arrays are stored in the output file attributes\n",
    "    as \"X_min\" and \"X_max\".\n",
    "\n",
    "    Parameters:\n",
    "        input_file  (str): Path to the existing HDF5 file.\n",
    "        output_file (str): Path to the new HDF5 file to create.\n",
    "        chunk_size  (int): Number of samples (windows) to process at a time\n",
    "                           when computing min and max, and when writing data.\n",
    "    \"\"\"\n",
    "\n",
    "    with h5py.File(input_file, \"r\") as f_in:\n",
    "        X_in = f_in[\"X\"]\n",
    "        num_windows, window_size, num_features = X_in.shape\n",
    "\n",
    "        # Check for optional datasets\n",
    "        y_in = f_in[\"y\"] if \"y\" in f_in else None\n",
    "        meta_in = f_in[\"meta\"] if \"meta\" in f_in else None\n",
    "\n",
    "        # --- 1) Compute global min and max across the entire \"X\" ---\n",
    "        # Initialize with opposite infinities\n",
    "        X_min = np.full((num_features,), np.inf, dtype=np.float32)\n",
    "        X_max = np.full((num_features,), -np.inf, dtype=np.float32)\n",
    "\n",
    "        # Pass A: find min and max feature-wise\n",
    "        for start_idx in range(0, num_windows, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, num_windows)\n",
    "            chunk = X_in[start_idx:end_idx]  # shape: (chunk_size, window_size, num_features)\n",
    "            # Flatten (chunk_size*window_size) x num_features\n",
    "            chunk_2d = chunk.reshape(-1, num_features)\n",
    "            chunk_min = chunk_2d.min(axis=0)\n",
    "            chunk_max = chunk_2d.max(axis=0)\n",
    "\n",
    "            X_min = np.minimum(X_min, chunk_min)\n",
    "            X_max = np.maximum(X_max, chunk_max)\n",
    "\n",
    "        # Protect against zero range in scaling (when max == min)\n",
    "        # We'll do an epsilon-based approach so no divide-by-zero.\n",
    "        epsilon = 1e-12\n",
    "        X_range = np.maximum(X_max - X_min, epsilon)\n",
    "\n",
    "        # --- 2) Create output file and datasets ---\n",
    "        with h5py.File(output_file, \"w\") as f_out:\n",
    "            # Copy the attributes from the input file if you want\n",
    "            for attr_key, attr_val in f_in.attrs.items():\n",
    "                f_out.attrs[attr_key] = attr_val\n",
    "\n",
    "            # Also store the computed min/max as attributes\n",
    "            f_out.attrs[\"X_min\"] = X_min\n",
    "            f_out.attrs[\"X_max\"] = X_max\n",
    "\n",
    "            # Create scaled X dataset\n",
    "            X_out = f_out.create_dataset(\n",
    "                \"X\",\n",
    "                shape=(num_windows, window_size, num_features),\n",
    "                dtype=np.float32,\n",
    "                compression=\"gzip\",\n",
    "                chunks=(chunk_size, window_size, num_features)\n",
    "            )\n",
    "\n",
    "            # If \"y\" exists, copy it\n",
    "            if y_in is not None:\n",
    "                y_out = f_out.create_dataset(\n",
    "                    \"y\",\n",
    "                    data=y_in[:],\n",
    "                    dtype=y_in.dtype,\n",
    "                    compression=\"gzip\",\n",
    "                    chunks=True\n",
    "                )\n",
    "                # Optionally replicate y_in.attrs if any\n",
    "                for ak, av in y_in.attrs.items():\n",
    "                    y_out.attrs[ak] = av\n",
    "\n",
    "            # If \"meta\" exists, copy it\n",
    "            if meta_in is not None:\n",
    "                meta_out = f_out.create_dataset(\n",
    "                    \"meta\",\n",
    "                    data=meta_in[:],\n",
    "                    dtype=meta_in.dtype,\n",
    "                    compression=\"gzip\",\n",
    "                    chunks=True\n",
    "                )\n",
    "                # Optionally replicate meta_in.attrs if any\n",
    "                for ak, av in meta_in.attrs.items():\n",
    "                    meta_out.attrs[ak] = av\n",
    "\n",
    "            # Copy dataset-level attributes for \"X\" (like feature_columns)\n",
    "            for ak, av in X_in.attrs.items():\n",
    "                X_out.attrs[ak] = av\n",
    "\n",
    "            # --- 3) Pass B: read + scale + write ---\n",
    "            for start_idx in range(0, num_windows, chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, num_windows)\n",
    "                chunk = X_in[start_idx:end_idx]\n",
    "                # chunk: shape (chunk_size, window_size, num_features)\n",
    "\n",
    "                # Scale feature-wise:\n",
    "                # scaled = (value - X_min) / (X_max - X_min)\n",
    "                # We'll do broadcast: (chunk - X_min) / X_range\n",
    "                scaled = (chunk - X_min) / X_range\n",
    "                X_out[start_idx:end_idx] = scaled\n",
    "\n",
    "        print(f\"✅ Finished min-max scaling. Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abdaa1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished min-max scaling. Output saved to ../../engine_windows_sample_scaled.h5\n"
     ]
    }
   ],
   "source": [
    "min_max_scale_hdf5(\n",
    "    input_file=\"../../engine_train_windows_sample.h5\",\n",
    "    output_file=\"../../engine_windows_sample_scaled.h5\",\n",
    "    chunk_size=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c67c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
