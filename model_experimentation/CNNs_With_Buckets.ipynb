{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef3f6c9",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We need datasets, dataloaders, and code to train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RULDataset(Dataset):\n",
    "    def __init__(self, X, y, dim=\"1d\"):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by passing a DataFrame.\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame with pixel data and labels.\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  \n",
    "        self.dim = dim\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample from the dataset.\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        Returns:\n",
    "            (torch.Tensor, torch.Tensor): Tuple of features and label.\n",
    "        \"\"\"\n",
    "        X_sample = self.X[idx]\n",
    "        y_sample = self.y[idx]\n",
    "\n",
    "        if self.dim == \"2d\":\n",
    "            X_sample = X_sample.unsqueeze(0)\n",
    "            \n",
    "        return X_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf505b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=10):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, loss):\n",
    "        if loss < self.min_loss:\n",
    "            self.min_loss = loss\n",
    "            self.counter = 0\n",
    "        elif loss > (self.min_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_dataloaders(X, y, test_size=0.2, batch_size=64, shuffle=True, dim=\"1d\", max_samples_per_class=20000, bucket_size=10):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into train and test sets and create DataLoaders.\n",
    "    Args:\n",
    "        X (np.ndarray): Input features.\n",
    "        y (np.ndarray): Target labels.\n",
    "        test_size (float): Proportion of the data to be used as test data.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        shuffle (bool): Whether to shuffle the data.\n",
    "        dim (str): Dimension type for the dataset.\n",
    "        max_samples_per_class (int): Maximum samples to take for each unique y value.\n",
    "    Returns:\n",
    "        tuple: (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Collect indices for each unique y value\n",
    "    class_samples = defaultdict(list)\n",
    "    \n",
    "    for idx in range(len(y)):\n",
    "        class_samples[y[idx]].append(idx)  # Store the index of the sample\n",
    "\n",
    "    # Create a list to hold the limited indices\n",
    "    limited_indices = []\n",
    "\n",
    "    for class_label, indices in class_samples.items():\n",
    "        limited_indices.extend(indices[:max_samples_per_class])  # Take the first 20,000 samples\n",
    "\n",
    "    # Bucket y values\n",
    "    bucketed_y = np.floor(y).astype(int)  \n",
    "    bucketed_y = (y // 10) \n",
    "\n",
    "    # Create a subset of the dataset with the limited indices\n",
    "    limited_X = X[limited_indices]\n",
    "    limited_y = bucketed_y[limited_indices]\n",
    "\n",
    "    # Shuffle the limited dataset if required\n",
    "    if shuffle:\n",
    "        shuffle_idxs = np.random.permutation(len(limited_y))\n",
    "        limited_X = limited_X[shuffle_idxs]\n",
    "        limited_y = limited_y[shuffle_idxs]\n",
    "\n",
    "    # Split the limited dataset into train and test sets\n",
    "    split_idx = int(len(limited_y) * (1 - test_size))\n",
    "    X_train, X_test = limited_X[:split_idx], limited_X[split_idx:]\n",
    "    y_train, y_test = limited_y[:split_idx], limited_y[split_idx:]\n",
    "\n",
    "    # Create train and test datasets\n",
    "    train_dataset = RULDataset(X_train, y_train, dim=dim)\n",
    "    test_dataset = RULDataset(X_test, y_test, dim=dim)\n",
    "\n",
    "    # Create DataLoaders for train and test sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_hdf5(filename=\"data.h5\"):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        X = f[\"X\"][:]\n",
    "        y = f[\"y\"][:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433bf771",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_from_hdf5(filename=\"../sample_windows.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device, print_loss=True):\n",
    "    model.eval()\n",
    "    overall_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Dynamically adjust input shape based on model type\n",
    "            if isinstance(model, CNNRUL):\n",
    "                inputs = inputs.permute(0, 2, 1)\n",
    "\n",
    "            outputs = model(inputs).squeeze()\n",
    "            targets = targets.long() \n",
    "            loss = criterion(outputs, targets)\n",
    "            overall_loss += loss.item()\n",
    "\n",
    "    avg_loss = overall_loss / len(data_loader)\n",
    "\n",
    "    if print_loss:\n",
    "        print(f\"Test MSE: {avg_loss:.4f}\")\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=5, track_test_loss=True):\n",
    "    model.to(device)\n",
    "    history = {'train_loss': [], 'test_loss': []}  \n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=.01)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Adjust shape based on model type\n",
    "            if isinstance(model, CNNRUL):  # 1D CNN\n",
    "                inputs = inputs.permute(0, 2, 1)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            targets = targets.long() \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        if track_test_loss:\n",
    "            avg_test_loss = evaluate_model(model, test_loader, criterion, device, print_loss=False)\n",
    "            history['test_loss'].append(avg_test_loss)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "            if early_stopper.early_stop(avg_test_loss):             \n",
    "                break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            if early_stopper.early_stop(avg_train_loss):             \n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_loss(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history['test_loss'], label='Test Loss')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs. Test Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27fb58f",
   "metadata": {},
   "source": [
    "# 1d CNN\n",
    "First, let's try a one dimensional CNN to capture temporal relationships within each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c95699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRUL(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CNNRUL, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * (50 // 2 // 2 // 2), 64)  # Adjust based on pooling\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.bar(unique_values, counts)\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"Y Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Frequency of Y Values\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29039d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting = np.count_nonzero(y == 3)\n",
    "\n",
    "counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_train_test_dataloaders(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b677e-c89d-4810-8b89-a8133966dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "\n",
    "# Assuming you have a DataLoader named `train_loader`\n",
    "for inputs, targets in train_loader:\n",
    "    unique_labels.update(targets.cpu().numpy())  # Use update to add multiple values\n",
    "\n",
    "print(unique_labels)  # This will print unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e097c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c99b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = 44\n",
    "model = CNNRUL(num_features=num_feats).to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "num_epochs = 10 \n",
    "\n",
    "history = train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, targets):\n",
    "    _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "    correct_predictions = (predicted == targets).sum().item()\n",
    "    return correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5363a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "predicted_values = []\n",
    "correct_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move to the appropriate device\n",
    "        \n",
    "        # Adjust input shape if necessary\n",
    "        if isinstance(model, CNNRUL):\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "\n",
    "        outputs = model(inputs).squeeze()  # Get model predictions\n",
    "\n",
    "        # Calculate accuracy using the provided function\n",
    "        correct_predictions += calculate_accuracy(outputs, targets)\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "        # Store predicted values and correct labels\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        predicted_values.extend(predicted.cpu().numpy())\n",
    "        correct_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = correct_predictions / total_predictions * 100  # Convert to percentage\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(correct_labels, predicted_values)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(correct_labels), yticklabels=np.unique(correct_labels))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01153a7",
   "metadata": {},
   "source": [
    "# 2d CNN\n",
    "Now, let's try out a two dimensional CNN to capture interactions among features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ece6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRUL2D(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CNNRUL2D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2)) \n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * (50 // 2 // 2 // 2) * (num_features // 2 // 2 // 2), 64)\n",
    "        self.fc2 = nn.Linear(64, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch, 1, 50, 44]\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten and pass through FC layers\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_train_test_dataloaders(X, y, dim=\"2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbecca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = 44\n",
    "model = CNNRUL2D(num_features=num_feats).to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "num_epochs = 10 \n",
    "\n",
    "history = train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750be3fa",
   "metadata": {},
   "source": [
    "# Hybrid CNN\n",
    "Now, let's try a hybrid CNN that uses both 1d and 2d convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCNN(nn.Module):\n",
    "    def __init__(self, num_features, seq_length):\n",
    "        super(HybridCNN, self).__init__()\n",
    "\n",
    "        # 1d CNN Branch (Temporal Patterns)\n",
    "        self.conv1d = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool1d = nn.MaxPool1d(kernel_size=2)  \n",
    "\n",
    "        # 2d CNN Branch (Feature Interactions)\n",
    "        self.conv2d = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.pool2d = nn.MaxPool2d(kernel_size=(2, 2))  \n",
    "\n",
    "        # Compute Fully Connected Layer Sizes\n",
    "        fc1d_input_size = 64 * (seq_length // 2)  # After 1D CNN\n",
    "        fc2d_input_size = 32 * ((seq_length // 2) * (num_features // 2))  # After 2D CNN\n",
    "\n",
    "        self.fc1 = nn.Linear(fc1d_input_size + fc2d_input_size, 128)  \n",
    "        self.fc2 = nn.Linear(128, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1d CNN Branch\n",
    "        x1d = x.permute(0, 2, 1)  # Convert [batch, seq_length, features] â†’ [batch, features, seq_length]\n",
    "        x1d = self.conv1d(x1d) \n",
    "        x1d = self.pool1d(torch.relu(x1d)) \n",
    "        x1d = torch.flatten(x1d, start_dim=1)  \n",
    "\n",
    "        # 2d CNN Branch\n",
    "        x2d = x.unsqueeze(1)  # Convert [batch, seq_length, features] â†’ [batch, 1, seq_length, features]\n",
    "        x2d = self.conv2d(x2d)  \n",
    "        x2d = self.pool2d(torch.relu(x2d))  \n",
    "        x2d = torch.flatten(x2d, start_dim=1) \n",
    "\n",
    "        # Combine both branches\n",
    "        x_combined = torch.cat((x1d, x2d), dim=1)\n",
    "\n",
    "        x_combined = torch.relu(self.fc1(x_combined))\n",
    "        return self.fc2(x_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_train_test_dataloaders(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb949ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = 44\n",
    "seq_length = 50  \n",
    "model = HybridCNN(num_features=num_feats, seq_length=seq_length).to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "num_epochs = 10\n",
    "\n",
    "history = train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b9fcc-f9c9-46fe-b56f-23e03bb4ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def plot_rul_predictions(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Plots predicted RUL vs. actual RUL for the test set.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model for RUL prediction.\n",
    "        test_loader (DataLoader): DataLoader containing test data.\n",
    "        device (str): 'cuda' or 'cpu' based on availability.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    actual_rul = []\n",
    "    predicted_rul = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Ensure the input shape is correct for different models\n",
    "            if isinstance(model, TCN):  # TCN requires (batch, features, seq_len)\n",
    "                inputs = inputs.permute(0, 2, 1)  \n",
    "            \n",
    "            outputs = model(inputs)  # Get predictions (now shape: [batch, 10])\n",
    "            \n",
    "            if isinstance(model.fc2, torch.nn.Linear) and model.fc2.out_features == 10:\n",
    "                # take the most probable bucket\n",
    "                predicted_rul.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "            actual_rul.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Convert to NumPy arrays for plotting\n",
    "    actual_rul = np.array(actual_rul)\n",
    "    predicted_rul = np.array(predicted_rul)\n",
    "\n",
    "    # ðŸ”¹ Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(actual_rul, predicted_rul, alpha=0.5, label=\"Predicted vs Actual\", color=\"blue\")\n",
    "    plt.plot([min(actual_rul), max(actual_rul)], [min(actual_rul), max(actual_rul)], 'r--', label=\"Perfect Prediction\")  \n",
    "    plt.xlabel(\"Actual RUL\")\n",
    "    plt.ylabel(\"Predicted RUL\")\n",
    "    plt.title(\"Predicted vs. Actual RUL\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3345f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rul_predictions(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa7bc1",
   "metadata": {},
   "source": [
    "# More Complex Hybrid Model\n",
    "The model's loss dropping drastically and then remaining stable might indicate underfitting. So, let's try adding additional convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HybridCNN(nn.Module):\n",
    "    def __init__(self, num_features, seq_length):\n",
    "        super(HybridCNN, self).__init__()\n",
    "\n",
    "        # 1d CNN Branch (Temporal Feature Extraction)\n",
    "        self.conv1d_1 = nn.Conv1d(in_channels=num_features, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv1d_2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1d_3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool1d = nn.MaxPool1d(kernel_size=2)  \n",
    "        self.bn1d_1 = nn.BatchNorm1d(32)\n",
    "        self.bn1d_2 = nn.BatchNorm1d(64)\n",
    "        self.bn1d_3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        ## 2d CNN Branch (Feature Interactions)\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1)\n",
    "        self.pool2d = nn.MaxPool2d(kernel_size=(2, 2))  \n",
    "        self.bn2d_1 = nn.BatchNorm2d(16)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(32)\n",
    "        self.bn2d_3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        fc1d_input_size = 128 * (seq_length // 8)  # After three 1D CNN pooling layers\n",
    "        fc2d_input_size = 64 * ((seq_length // 8) * (num_features // 8))  # After three 2D CNN pooling layers\n",
    "        combined_fc_size = fc1d_input_size + fc2d_input_size\n",
    "\n",
    "        self.fc1 = nn.Linear(combined_fc_size, 256)  \n",
    "        self.fc2 = nn.Linear(256, 128)  \n",
    "        self.fc3 = nn.Linear(128, 10)  \n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1d CNN Branch\n",
    "        x1d = x.permute(0, 2, 1)  # Convert [batch, seq_length, features] â†’ [batch, features, seq_length]\n",
    "        x1d = self.pool1d(torch.relu(self.bn1d_1(self.conv1d_1(x1d))))\n",
    "        x1d = self.pool1d(torch.relu(self.bn1d_2(self.conv1d_2(x1d))))\n",
    "        x1d = self.pool1d(torch.relu(self.bn1d_3(self.conv1d_3(x1d))))\n",
    "        x1d = torch.flatten(x1d, start_dim=1)  \n",
    "\n",
    "        # 2d CNN Branch\n",
    "        x2d = x.unsqueeze(1)  # Convert [batch, seq_length, features] â†’ [batch, 1, seq_length, features]\n",
    "        x2d = self.pool2d(torch.relu(self.bn2d_1(self.conv2d_1(x2d))))\n",
    "        x2d = self.pool2d(torch.relu(self.bn2d_2(self.conv2d_2(x2d))))\n",
    "        x2d = self.pool2d(torch.relu(self.bn2d_3(self.conv2d_3(x2d))))\n",
    "        x2d = torch.flatten(x2d, start_dim=1)  \n",
    "\n",
    "        # Combine both branches\n",
    "        x_combined = torch.cat((x1d, x2d), dim=1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x_combined = torch.relu(self.fc1(x_combined))\n",
    "        x_combined = self.dropout(x_combined)\n",
    "        x_combined = torch.relu(self.fc2(x_combined))\n",
    "        x_combined = self.dropout(x_combined)\n",
    "\n",
    "        return self.fc3(x_combined)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = 44\n",
    "seq_length = 50  \n",
    "model = HybridCNN(num_features=num_feats, seq_length=seq_length).to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "num_epochs = 10\n",
    "\n",
    "history = train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723f164",
   "metadata": {},
   "source": [
    "# Temporal CNN\n",
    "Our more complex hybrid CNN was massively overfitting, so let's go in a different direction. Let's try a temporal CNN instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block in a Temporal Convolutional Network (TCN).\n",
    "    - Uses dilated causal convolutions, batch normalization, and residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "\n",
    "        padding = (kernel_size - 1) * dilation // 2  # ðŸ”¹ Ensures output shape matches input\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               padding=padding, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               padding=padding, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ðŸ”¹ Adjust residual connection if the number of channels changes\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x if self.downsample is None else self.downsample(x)  # ðŸ”¹ Match channel size if needed\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x + res  # ðŸ”¹ Ensures shape matches\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network (TCN) for RUL Prediction.\n",
    "    - Uses multiple TemporalBlocks with increasing dilation.\n",
    "    - Outputs a single regression value per sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, seq_length, num_channels=[64, 128, 256], kernel_size=3, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "\n",
    "        for i in range(num_levels):\n",
    "            dilation = 2 ** i  # ðŸ”¹ Exponentially increasing dilation rate\n",
    "            in_channels = num_features if i == 0 else num_channels[i - 1]\n",
    "            out_channels = num_channels[i]\n",
    "\n",
    "            layers.append(TemporalBlock(in_channels, out_channels, kernel_size, dilation, dropout))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1] * seq_length, 1)  # Fully connected output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [batch, seq_length, num_features]\n",
    "        Expected shape: [batch, num_features, seq_length]\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1)  # ðŸ”¹ Convert [batch, seq_length, num_features] â†’ [batch, num_features, seq_length]\n",
    "        x = self.network(x)\n",
    "        x = x.view(x.shape[0], -1)  # Flatten for FC layer\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = 44\n",
    "seq_length = 50\n",
    "model = TCN(num_features=num_feats, seq_length=seq_length).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "num_epochs = 20\n",
    "\n",
    "history = train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836bc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
