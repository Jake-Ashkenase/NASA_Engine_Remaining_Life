{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a94a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e6a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"N-CMAPSS_DS01-005.h5\", \"N-CMAPSS_DS02-006.h5\", \"N-CMAPSS_DS03-012.h5\", \"N-CMAPSS_DS04.h5\", \"N-CMAPSS_DS05.h5\", \"N-CMAPSS_DS06.h5\", \"N-CMAPSS_DS07.h5\", \"N-CMAPSS_DS08a-009.h5\", \"N-CMAPSS_DS08c-008.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588843b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(filename):\n",
    "    with h5py.File(filename, 'r') as infile:\n",
    "        # get columns from file\n",
    "        W_var = np.array(infile.get('W_var'))\n",
    "        X_s_var = np.array(infile.get('X_s_var'))  \n",
    "        X_v_var = np.array(infile.get('X_v_var')) \n",
    "        T_var = np.array(infile.get('T_var'))\n",
    "        A_var = np.array(infile.get('A_var'))\n",
    "\n",
    "    # from np.array to list dtype U4/U5\n",
    "    W_var = list(np.array(W_var, dtype='U20'))\n",
    "    X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "    X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "    T_var = list(np.array(T_var, dtype='U20'))\n",
    "    A_var = list(np.array(A_var, dtype='U20'))\n",
    "\n",
    "    return W_var + X_s_var + X_v_var + T_var + A_var \n",
    "\n",
    "def extract_data(filename, get_cols=False):\n",
    "    with h5py.File(filename, 'r') as infile:\n",
    "        # train data\n",
    "        W_dev = np.array(infile.get('W_dev'))          \n",
    "        X_s_dev = np.array(infile.get('X_s_dev'))       \n",
    "        X_v_dev = np.array(infile.get('X_v_dev'))      \n",
    "        T_dev = np.array(infile.get('T_dev'))       \n",
    "        y_train = np.array(infile.get('Y_dev'))           \n",
    "        A_dev = np.array(infile.get('A_dev')) \n",
    "        x_train = np.concatenate((W_dev, X_s_dev, X_v_dev, T_dev, A_dev), axis=1)\n",
    "\n",
    "        # test data\n",
    "        W_test = np.array(infile.get('W_test'))        \n",
    "        X_s_test = np.array(infile.get('X_s_test'))     \n",
    "        X_v_test = np.array(infile.get('X_v_test'))      \n",
    "        T_test = np.array(infile.get('T_test'))         \n",
    "        y_test = np.array(infile.get('Y_test'))         \n",
    "        A_test = np.array(infile.get('A_test')) \n",
    "        x_test = np.concatenate((W_test, X_s_test, X_v_test, T_test, A_test), axis=1)\n",
    "\n",
    "    # concat data together for EDA\n",
    "    #X = np.concatenate((x_train, x_test), axis=0)\n",
    "    #y = np.concatenate((y_train, y_test), axis=0)\n",
    "    \n",
    "    # possibly extract columns\n",
    "    cols = None\n",
    "    if get_cols:\n",
    "        cols = extract_columns(filename)\n",
    "\n",
    "    return {\"data\": (x_train, x_test, y_train, y_test), \"columns\": cols}\n",
    "\n",
    "def read_data(files):\n",
    "    data = extract_data(files[0], get_cols=True)\n",
    "    columns = data[\"columns\"]\n",
    "    X, y = data[\"data\"]\n",
    "    for filename in files[1:]:\n",
    "        X_temp, y_temp = extract_data(filename)[\"data\"]\n",
    "        X, y = np.concatenate((X, X_temp), axis=0), np.concatenate((y, y_temp), axis=0)\n",
    "    return X, y, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb7e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(files, output_file, id_cols, chunk_size=10000):\n",
    "    train_file = output_file + \"_train.h5\"\n",
    "    test_file = output_file + \"_test.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    train_size, test_size = 0, 0\n",
    "\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        data = extract_data(filename, get_cols=i==0)  # Read one file at a time\n",
    "        X_train, X_test, y_train, y_test = data[\"data\"]\n",
    "        y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "\n",
    "        if i == 0:\n",
    "            columns = data[\"columns\"]\n",
    "            id_idxs = [columns.index(col) for col in id_cols]\n",
    "            # make starting datasets\n",
    "            h5_train.create_dataset(\"X\", shape=(0, X_train.shape[1]), maxshape=(None, X_train.shape[1]), dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_train.shape[1]))\n",
    "            h5_train.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", chunks=(chunk_size,))\n",
    "\n",
    "            h5_test.create_dataset(\"X\", shape=(0, X_test.shape[1]), maxshape=(None, X_test.shape[1]), dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_test.shape[1]))\n",
    "            h5_test.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", chunks=(chunk_size,))\n",
    "\n",
    "            h5_train.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "            h5_test.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "\n",
    "\n",
    "\n",
    "        # append data incrementally to avoid memory issues\n",
    "        # fix shapes\n",
    "        h5_train[\"X\"].resize((train_size + X_train.shape[0]), axis=0)\n",
    "        h5_train[\"X\"][train_size:] = X_train\n",
    "        h5_train[\"y\"].resize((train_size + y_train.shape[0]), axis=0)\n",
    "        h5_train[\"y\"][train_size:] = y_train\n",
    "\n",
    "        h5_test[\"X\"].resize((test_size + X_test.shape[0]), axis=0)\n",
    "        h5_test[\"X\"][test_size:] = X_test\n",
    "        h5_test[\"y\"].resize((test_size + y_test.shape[0]), axis=0)\n",
    "        h5_test[\"y\"][test_size:] = y_test\n",
    "\n",
    "        train_size += X_train.shape[0]\n",
    "        test_size += X_test.shape[0]\n",
    "\n",
    "    # cleanup\n",
    "    h5_train.close()\n",
    "    h5_test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d559cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N-CMAPSS_DS01-005.h5 (1/9)...\n",
      "Processing N-CMAPSS_DS02-006.h5 (2/9)...\n",
      "Processing N-CMAPSS_DS03-012.h5 (3/9)...\n",
      "Processing N-CMAPSS_DS04.h5 (4/9)...\n",
      "Processing N-CMAPSS_DS05.h5 (5/9)...\n",
      "Processing N-CMAPSS_DS06.h5 (6/9)...\n",
      "Processing N-CMAPSS_DS07.h5 (7/9)...\n",
      "Processing N-CMAPSS_DS08a-009.h5 (8/9)...\n",
      "Processing N-CMAPSS_DS08c-008.h5 (9/9)...\n"
     ]
    }
   ],
   "source": [
    "merge_files(files, \"engine_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901f382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(files, output_file, id_cols, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Merges multiple HDF5 files into a single train/test HDF5 dataset and creates an index.csv file.\n",
    "\n",
    "    Parameters:\n",
    "        files (list): List of HDF5 file paths to merge.\n",
    "        output_file (str): Output HDF5 file path (without extension).\n",
    "        id_cols (list): List of column names identifying a time series (e.g., [\"unit\", \"cycle\"]).\n",
    "        chunk_size (int): Number of rows to process in each batch (to prevent memory issues).\n",
    "    \"\"\"\n",
    "    train_file = output_file + \"_train.h5\"\n",
    "    test_file = output_file + \"_test.h5\"\n",
    "    h5_train = h5py.File(train_file, \"w\")\n",
    "    h5_test = h5py.File(test_file, \"w\")\n",
    "\n",
    "    train_size, test_size = 0, 0\n",
    "    index_data = []  # Store (unit, cycle, start, stop, dataset_type)\n",
    "\n",
    "    for i, filename in enumerate(files):\n",
    "        print(f\"Processing {filename} ({i+1}/{len(files)})...\")\n",
    "\n",
    "        data = extract_data(filename, get_cols=(i == 0))  # Read one file at a time\n",
    "        X_train, X_test, y_train, y_test = data[\"data\"]\n",
    "        y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "\n",
    "        if i == 0:\n",
    "            columns = data[\"columns\"]\n",
    "\n",
    "            # Create datasets\n",
    "            h5_train.create_dataset(\"X\", shape=(0, X_train.shape[1]), maxshape=(None, X_train.shape[1]), \n",
    "                                    dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_train.shape[1]))\n",
    "            h5_train.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", \n",
    "                                    chunks=(chunk_size,))\n",
    "\n",
    "            h5_test.create_dataset(\"X\", shape=(0, X_test.shape[1]), maxshape=(None, X_test.shape[1]), \n",
    "                                   dtype='float32', compression=\"gzip\", chunks=(chunk_size, X_test.shape[1]))\n",
    "            h5_test.create_dataset(\"y\", shape=(0,), maxshape=(None,), dtype='float32', compression=\"gzip\", \n",
    "                                   chunks=(chunk_size,))\n",
    "\n",
    "            h5_train.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "            h5_test.attrs[\"columns\"] = np.array(columns, dtype=\"S\")\n",
    "\n",
    "        # ** Extract indices of ID columns **\n",
    "        id_idxs = [columns.index(col) for col in id_cols]\n",
    "\n",
    "        # ** Extract unique (unit, cycle) time series for train and test sets **\n",
    "        for dataset_type, X_data, base_index in [(\"train\", X_train, train_size), (\"test\", X_test, test_size)]:\n",
    "            if X_data.shape[0] == 0:\n",
    "                continue  # Skip if there's no data\n",
    "\n",
    "            # Load only unit and cycle columns\n",
    "            id_values = X_data[:, id_idxs]\n",
    "            id_df = pd.DataFrame(id_values, columns=id_cols)\n",
    "\n",
    "            # Get start and stop indices for each unique (unit, cycle)\n",
    "            grouped = id_df.groupby(id_cols).apply(lambda df: (df.index.min(), df.index.max()))\n",
    "            for (unit, cycle), (start, stop) in grouped.items():\n",
    "                index_data.append([unit, cycle, start + base_index, stop + base_index, dataset_type])\n",
    "\n",
    "        # ** Append data incrementally to avoid memory issues **\n",
    "        h5_train[\"X\"].resize((train_size + X_train.shape[0]), axis=0)\n",
    "        h5_train[\"X\"][train_size:] = X_train\n",
    "        h5_train[\"y\"].resize((train_size + y_train.shape[0]), axis=0)\n",
    "        h5_train[\"y\"][train_size:] = y_train\n",
    "\n",
    "        h5_test[\"X\"].resize((test_size + X_test.shape[0]), axis=0)\n",
    "        h5_test[\"X\"][test_size:] = X_test\n",
    "        h5_test[\"y\"].resize((test_size + y_test.shape[0]), axis=0)\n",
    "        h5_test[\"y\"][test_size:] = y_test\n",
    "\n",
    "        train_size += X_train.shape[0]\n",
    "        test_size += X_test.shape[0]\n",
    "\n",
    "    # ** Save the index file as CSV **\n",
    "    index_df = pd.DataFrame(index_data, columns=[\"unit\", \"cycle\", \"start_idx\", \"stop_idx\", \"dataset\"])\n",
    "    index_df.to_csv(output_file + \"_index.csv\", index=False)\n",
    "    print(f\"✅ Index file saved: {output_file}_index.csv\")\n",
    "\n",
    "    # Cleanup\n",
    "    h5_train.close()\n",
    "    h5_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84914e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N-CMAPSS_DS01-005.h5 (1/9)...\n",
      "Processing N-CMAPSS_DS02-006.h5 (2/9)...\n",
      "Processing N-CMAPSS_DS03-012.h5 (3/9)...\n",
      "Processing N-CMAPSS_DS04.h5 (4/9)...\n",
      "Processing N-CMAPSS_DS05.h5 (5/9)...\n",
      "Processing N-CMAPSS_DS06.h5 (6/9)...\n",
      "Processing N-CMAPSS_DS07.h5 (7/9)...\n",
      "Processing N-CMAPSS_DS08a-009.h5 (8/9)...\n",
      "Processing N-CMAPSS_DS08c-008.h5 (9/9)...\n",
      "✅ Index file saved: engine_index.csv\n"
     ]
    }
   ],
   "source": [
    "merge_files(files, \"engine\", id_cols=[\"unit\", \"cycle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd7f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
